"""
åˆå¹¶å¤šä¸ªLeRobotæ•°æ®é›†ä¸ºå•ä¸ªåŒ…å«å¤šä¸ªepisodesçš„æ•°æ®é›†
ç”¨æ³•: python merge_datasets.py
"""

import pandas as pd
import json
import shutil
from pathlib import Path
from datetime import datetime
import cv2


def merge_lerobot_datasets(dataset_folders, output_name=None):
    """
    åˆå¹¶å¤šä¸ªLeRobotæ•°æ®é›†æ–‡ä»¶å¤¹ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ªepisodesçš„æ•°æ®é›†
    
    Args:
        dataset_folders: è¦åˆå¹¶çš„æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
        output_name: è¾“å‡ºæ•°æ®é›†åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    """
    
    if not dataset_folders:
        print("âŒ æ²¡æœ‰æä¾›æ•°æ®é›†æ–‡ä»¶å¤¹")
        return None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_name is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_name = f"lerobot_dataset_merged_{timestamp}"
    
    output_path = Path(output_name)
    if output_path.exists():
        print(f"âš ï¸  è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_path}")
        response = input("æ˜¯å¦è¦†ç›–? (y/n): ")
        if response.lower() != 'y':
            print("å–æ¶ˆåˆå¹¶")
            return None
        shutil.rmtree(output_path)
    
    output_path.mkdir(parents=True)
    (output_path / "data" / "chunk-000").mkdir(parents=True)
    (output_path / "meta").mkdir(parents=True)
    (output_path / "videos" / "observation.images.camera_color").mkdir(parents=True)
    (output_path / "videos" / "observation.images.camera_depth").mkdir(parents=True)
    
    print(f"\nğŸ“¦ å¼€å§‹åˆå¹¶ {len(dataset_folders)} ä¸ªæ•°æ®é›†...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    
    # æ”¶é›†æ‰€æœ‰æ•°æ®
    all_frames = []
    all_episodes = []
    all_tasks = set()
    
    current_episode_index = 0
    total_frames = 0
    
    for folder_idx, folder in enumerate(dataset_folders):
        folder_path = Path(folder)
        
        if not folder_path.exists():
            print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹: {folder}")
            continue
        
        print(f"\nå¤„ç† [{folder_idx + 1}/{len(dataset_folders)}]: {folder_path.name}")
        
        try:
            # è¯»å–å¸§æ•°æ®
            data_file = folder_path / "data" / "chunk-000" / "file-000000.parquet"
            if not data_file.exists():
                print(f"  âš ï¸  æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_file}")
                continue
            
            df = pd.read_parquet(data_file)
            print(f"  âœ“ è¯»å– {len(df)} å¸§æ•°æ®")
            
            # è¯»å–episodeå…ƒæ•°æ®
            episodes_file = folder_path / "meta" / "episodes.parquet"
            if episodes_file.exists():
                episodes_df = pd.read_parquet(episodes_file)
                
                # å¤„ç†æ¯ä¸ªepisode
                for _, episode in episodes_df.iterrows():
                    old_ep_idx = int(episode['episode_index'])
                    
                    # è·å–è¯¥episodeçš„æ‰€æœ‰å¸§
                    episode_frames = df[df['episode_index'] == old_ep_idx].copy()
                    
                    if len(episode_frames) == 0:
                        continue
                    
                    # æ›´æ–°episodeç´¢å¼•
                    episode_frames['episode_index'] = current_episode_index
                    
                    # æ›´æ–°å…¨å±€ç´¢å¼•
                    episode_frames['index'] = range(total_frames, total_frames + len(episode_frames))
                    
                    # é‡ç½®frameç´¢å¼•
                    episode_frames['frame_index'] = range(len(episode_frames))
                    
                    # æ›´æ–°è§†é¢‘å¼•ç”¨ä¸­çš„episodeç¼–å·
                    for img_col in ['observation.images.camera_color', 'observation.images.camera_depth']:
                        if img_col in episode_frames.columns:
                            def update_video_path(val):
                                if isinstance(val, str):
                                    video_info = json.loads(val)
                                    old_path = video_info['path']
                                    # æ›´æ–°è·¯å¾„ä¸­çš„episodeç¼–å·
                                    new_path = old_path.replace(
                                        f"episode_{old_ep_idx:06d}",
                                        f"episode_{current_episode_index:06d}"
                                    )
                                    video_info['path'] = new_path
                                    return json.dumps(video_info)
                                return val
                            
                            episode_frames[img_col] = episode_frames[img_col].apply(update_video_path)
                    
                    # å¤åˆ¶è§†é¢‘æ–‡ä»¶
                    for video_type in ['observation.images.camera_color', 'observation.images.camera_depth']:
                        video_type_short = video_type.split('.')[-1]
                        old_video = folder_path / "videos" / video_type / f"episode_{old_ep_idx:06d}.mp4"
                        new_video = output_path / "videos" / video_type / f"episode_{current_episode_index:06d}.mp4"
                        
                        if old_video.exists():
                            shutil.copy2(old_video, new_video)
                            print(f"  âœ“ å¤åˆ¶è§†é¢‘: episode_{current_episode_index:06d}.mp4 ({video_type_short})")
                    
                    # æ·»åŠ åˆ°æ€»æ•°æ®
                    all_frames.append(episode_frames)
                    
                    # åˆ›å»ºæ–°çš„episodeå…ƒæ•°æ®
                    new_episode = {
                        'episode_index': current_episode_index,
                        'length': len(episode_frames),
                        'timestamp': datetime.now().isoformat(),
                        'tasks': episode.get('tasks', ['teach_drag']),
                        'data/chunk_index': 0,
                        'data/file_index': 0,
                        f'videos/{video_type}/chunk_index': 0,
                        f'videos/{video_type}/file_index': current_episode_index,
                        f'videos/{video_type}/from_timestamp': 0.0,
                        f'videos/{video_type}/to_timestamp': float((len(episode_frames) - 1) / 10),
                    }
                    
                    # æ·»åŠ ä¸¤ä¸ªè§†é¢‘ç±»å‹çš„å…ƒæ•°æ®
                    for video_type in ['observation.images.camera_color', 'observation.images.camera_depth']:
                        new_episode[f'videos/{video_type}/chunk_index'] = 0
                        new_episode[f'videos/{video_type}/file_index'] = current_episode_index
                        new_episode[f'videos/{video_type}/from_timestamp'] = 0.0
                        new_episode[f'videos/{video_type}/to_timestamp'] = float((len(episode_frames) - 1) / 10)
                    
                    all_episodes.append(new_episode)
                    
                    # æ”¶é›†ä»»åŠ¡
                    tasks = episode.get('tasks', ['teach_drag'])
                    if isinstance(tasks, list):
                        all_tasks.update(tasks)
                    
                    total_frames += len(episode_frames)
                    current_episode_index += 1
                    
                    print(f"  âœ“ Episode {current_episode_index - 1}: {len(episode_frames)} å¸§")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not all_frames:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®")
        return None
    
    print(f"\nğŸ“Š åˆå¹¶ç»Ÿè®¡:")
    print(f"  æ€»Episodes: {len(all_episodes)}")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    print(f"  ä»»åŠ¡ç±»å‹: {list(all_tasks)}")
    
    # åˆå¹¶æ‰€æœ‰å¸§æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ•°æ®...")
    merged_df = pd.concat(all_frames, ignore_index=True)
    
    # ä¿å­˜ä¸ºparquet
    output_data_file = output_path / "data" / "chunk-000" / "file-000000.parquet"
    merged_df.to_parquet(output_data_file, index=False)
    print(f"  âœ“ å¸§æ•°æ®å·²ä¿å­˜: {output_data_file}")
    
    # ä¿å­˜episodeså…ƒæ•°æ®
    episodes_df = pd.DataFrame(all_episodes)
    episodes_output = output_path / "meta" / "episodes.parquet"
    episodes_df.to_parquet(episodes_output, index=False)
    print(f"  âœ“ Episodeså…ƒæ•°æ®å·²ä¿å­˜: {episodes_output}")
    
    # è¯»å–ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„info.jsonä½œä¸ºæ¨¡æ¿
    first_folder = Path(dataset_folders[0])
    info_template = {}
    if (first_folder / "meta" / "info.json").exists():
        with open(first_folder / "meta" / "info.json", 'r', encoding='utf-8') as f:
            info_template = json.load(f)
    
    # æ›´æ–°info.json
    info_template['total_episodes'] = len(all_episodes)
    info_template['total_frames'] = total_frames
    
    info_output = output_path / "meta" / "info.json"
    with open(info_output, 'w', encoding='utf-8') as f:
        json.dump(info_template, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ Infoå·²ä¿å­˜: {info_output}")
    
    # ä¿å­˜tasks
    tasks_data = [{"task": task, "task_index": idx} for idx, task in enumerate(sorted(all_tasks))]
    tasks_df = pd.DataFrame(tasks_data)
    tasks_output = output_path / "meta" / "tasks.parquet"
    tasks_df.to_parquet(tasks_output, index=False)
    print(f"  âœ“ Taskså·²ä¿å­˜: {tasks_output}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
    stats = {}
    
    # æ•°å€¼ç‰¹å¾ç»Ÿè®¡
    numeric_features = [
        'observation.state.J1', 'observation.state.J2', 'observation.state.J3',
        'observation.state.J4', 'observation.state.J5', 'observation.state.J6',
        'observation.cartesian_position.X', 'observation.cartesian_position.Y',
        'observation.cartesian_position.Z', 'observation.cartesian_position.Rx',
        'observation.cartesian_position.Ry', 'observation.cartesian_position.Rz',
        'action.J1_vel', 'action.J2_vel', 'action.J3_vel',
        'action.J4_vel', 'action.J5_vel', 'action.J6_vel', 'action.gripper'
    ]
    
    for feature in numeric_features:
        if feature in merged_df.columns:
            stats[feature] = {
                "min": float(merged_df[feature].min()),
                "max": float(merged_df[feature].max()),
                "mean": float(merged_df[feature].mean()),
                "std": float(merged_df[feature].std())
            }
    
    stats_output = output_path / "meta" / "stats.json"
    with open(stats_output, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_output}")
    
    # åˆ›å»ºREADME
    readme_content = f"""---
license: apache-2.0
tags:
- LeRobot
- robotics
- dobot
- teach-drag
task_categories:
- robotics
---

# {output_name}

åˆå¹¶çš„LeRobotæ ¼å¼æœºå™¨äººæ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªç¤ºæ•™è½¨è¿¹ã€‚

## æ•°æ®é›†ä¿¡æ¯

- **æœºå™¨äººç±»å‹**: dobot_cr5
- **æ€»Episodes**: {len(all_episodes)}
- **æ€»å¸§æ•°**: {total_frames}
- **ä»»åŠ¡ç±»å‹**: {', '.join(sorted(all_tasks))}
- **åˆ›å»ºæ—¶é—´**: {datetime.now().isoformat()}

## ä½¿ç”¨æ–¹æ³•

```python
from lerobot.datasets.lerobot_dataset import LeRobotDataset

# åŠ è½½æ•°æ®é›†
dataset = LeRobotDataset("{output_name}")

# è·å–ä¸€å¸§æ•°æ®
frame = dataset[0]
print(frame.keys())
```

## æ•°æ®æ¥æº

è¯¥æ•°æ®é›†ç”±ä»¥ä¸‹ {len(dataset_folders)} ä¸ªæ•°æ®é›†åˆå¹¶è€Œæˆï¼š

"""
    
    for idx, folder in enumerate(dataset_folders):
        readme_content += f"{idx + 1}. {Path(folder).name}\n"
    
    readme_content += f"""
åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    readme_output = output_path / "README.md"
    with open(readme_output, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    print(f"  âœ“ READMEå·²åˆ›å»º: {readme_output}")
    
    print("\n" + "="*60)
    print("âœ… æ•°æ®é›†åˆå¹¶å®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path.absolute()}")
    print(f"ğŸ“Š Episodes: {len(all_episodes)}")
    print(f"ğŸ“Š æ€»å¸§æ•°: {total_frames}")
    print("="*60)
    
    return str(output_path.absolute())


def auto_find_datasets(base_dir="."):
    """è‡ªåŠ¨æŸ¥æ‰¾ç›®å½•ä¸‹çš„æ‰€æœ‰LeRobotæ•°æ®é›†"""
    base_path = Path(base_dir)
    datasets = []
    
    for item in sorted(base_path.glob("lerobot_dataset_*")):
        if item.is_dir() and not item.name.endswith("_merged"):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
            if (item / "data" / "chunk-000" / "file-000000.parquet").exists():
                datasets.append(str(item))
    
    return datasets


if __name__ == "__main__":
    print("="*60)
    print("LeRobot æ•°æ®é›†åˆå¹¶å·¥å…·")
    print("="*60)
    
    # è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®é›†
    print("\nğŸ” æœç´¢å½“å‰ç›®å½•ä¸‹çš„æ•°æ®é›†...")
    datasets = auto_find_datasets(".")
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†æ–‡ä»¶å¤¹")
        print("æç¤º: æ•°æ®é›†æ–‡ä»¶å¤¹åº”è¯¥ä»¥ 'lerobot_dataset_' å¼€å¤´")
        exit(1)
    
    print(f"\næ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†:")
    for idx, dataset in enumerate(datasets):
        folder_name = Path(dataset).name
        # è¯»å–åŸºæœ¬ä¿¡æ¯
        try:
            df = pd.read_parquet(Path(dataset) / "data" / "chunk-000" / "file-000000.parquet")
            num_frames = len(df)
            num_episodes = df['episode_index'].nunique()
            print(f"  [{idx + 1}] {folder_name}: {num_episodes} episode(s), {num_frames} å¸§")
        except:
            print(f"  [{idx + 1}] {folder_name}")
    
    # è¯¢é—®æ˜¯å¦åˆå¹¶æ‰€æœ‰æ•°æ®é›†
    print("\n" + "="*60)
    response = input(f"æ˜¯å¦åˆå¹¶æ‰€æœ‰ {len(datasets)} ä¸ªæ•°æ®é›†? (y/n): ")
    
    if response.lower() != 'y':
        print("å–æ¶ˆåˆå¹¶")
        exit(0)
    
    # æ‰§è¡Œåˆå¹¶
    output = merge_lerobot_datasets(datasets)
    
    if output:
        print(f"\nâœ… æˆåŠŸï¼åˆå¹¶åçš„æ•°æ®é›†å¯ç”¨äº pi0 è®­ç»ƒ")
        print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {output}")
